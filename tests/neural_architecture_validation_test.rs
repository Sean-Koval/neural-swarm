//! Neural Architecture Validation Tests
//!
//! Comprehensive tests for neural network architectures used in task decomposition,
//! including transformer models, decision networks, and reinforcement learning components.

#[cfg(test)]
mod neural_architecture_validation {
    use super::*;
    use crate::neural_task_decomposition_test::*;
    use std::collections::HashMap;
    use uuid::Uuid;

    /// Mock neural architecture components for testing
    pub struct MockTransformerModel {
        pub vocab_size: usize,
        pub hidden_size: usize,
        pub num_layers: usize,
        pub num_heads: usize,
        pub accuracy: f32,
    }

    impl MockTransformerModel {
        pub fn new(vocab_size: usize, hidden_size: usize, num_layers: usize, num_heads: usize) -> Self {
            Self {
                vocab_size,
                hidden_size,
                num_layers,
                num_heads,
                accuracy: 0.0,
            }
        }

        pub fn encode_task(&self, task: &Task) -> Vec<f32> {
            // Mock encoding: convert task to feature vector
            vec![
                task.complexity,
                task.dependencies.len() as f32 / 10.0,
                task.estimated_duration as f32 / 3600.0,
                task.resources.memory_mb as f32 / 8192.0,
                task.resources.cpu_cores / 8.0,
                task.required_capabilities.len() as f32 / 10.0,
                match task.priority {
                    TaskPriority::Critical => 1.0,
                    TaskPriority::High => 0.8,
                    TaskPriority::Medium => 0.5,
                    TaskPriority::Low => 0.2,
                },
            ]
        }

        pub fn decode_subtasks(&self, encoded: &[f32]) -> Vec<Task> {
            // Mock decoding: convert feature vector back to subtasks
            let num_subtasks = (encoded[0] * 5.0).max(1.0) as usize;
            
            (0..num_subtasks).map(|i| {
                Task {
                    id: format!("transformer_subtask_{}", i),
                    name: format!("Transformer Generated Subtask {}", i),
                    description: format!("Generated by transformer model"),
                    complexity: encoded[0] / num_subtasks as f32,
                    dependencies: if i > 0 { vec![format!("transformer_subtask_{}", i-1)] } else { vec![] },
                    priority: TaskPriority::Medium,
                    estimated_duration: 300,
                    required_capabilities: vec!["transformer_generated".to_string()],
                    resources: TaskResources {
                        memory_mb: 512,
                        cpu_cores: 1.0,
                        network_bandwidth: 100,
                        storage_gb: 10.0,
                    },
                }
            }).collect()
        }

        pub fn train_step(&mut self, task: &Task, expected_decomposition: &[Task]) -> f32 {
            // Mock training: simple accuracy calculation
            let encoded = self.encode_task(task);
            let predicted = self.decode_subtasks(&encoded);
            
            let accuracy = if predicted.len() == expected_decomposition.len() {
                0.8
            } else {
                0.6
            };
            
            self.accuracy = (self.accuracy + accuracy) / 2.0;
            accuracy
        }
    }

    /// Mock decision network for task decomposition choices
    pub struct MockDecisionNetwork {
        pub input_size: usize,
        pub hidden_layers: Vec<usize>,
        pub output_size: usize,
        pub weights: Vec<Vec<f32>>,
        pub decision_accuracy: f32,
    }

    impl MockDecisionNetwork {
        pub fn new(input_size: usize, hidden_layers: Vec<usize>, output_size: usize) -> Self {
            let mut weights = Vec::new();
            let mut prev_size = input_size;
            
            for &layer_size in &hidden_layers {
                weights.push(vec![0.5; prev_size * layer_size]);
                prev_size = layer_size;
            }
            weights.push(vec![0.5; prev_size * output_size]);
            
            Self {
                input_size,
                hidden_layers,
                output_size,
                weights,
                decision_accuracy: 0.0,
            }
        }

        pub fn forward(&self, input: &[f32]) -> Vec<f32> {
            // Mock forward pass
            let mut current = input.to_vec();
            
            for (i, &layer_size) in self.hidden_layers.iter().enumerate() {
                current = self.apply_layer(&current, &self.weights[i], layer_size);
            }
            
            // Output layer
            self.apply_layer(&current, self.weights.last().unwrap(), self.output_size)
        }

        fn apply_layer(&self, input: &[f32], weights: &[f32], output_size: usize) -> Vec<f32> {
            (0..output_size).map(|i| {
                let sum: f32 = input.iter().enumerate()
                    .map(|(j, &x)| x * weights[i * input.len() + j])
                    .sum();
                
                // ReLU activation
                sum.max(0.0)
            }).collect()
        }

        pub fn make_decision(&self, task_features: &[f32]) -> DecompositionStrategy {
            let output = self.forward(task_features);
            
            // Choose strategy based on output
            let max_index = output.iter()
                .enumerate()
                .max_by(|(_, a), (_, b)| a.partial_cmp(b).unwrap())
                .map(|(i, _)| i)
                .unwrap_or(0);
            
            match max_index {
                0 => DecompositionStrategy::Heuristic,
                1 => DecompositionStrategy::Neural,
                _ => DecompositionStrategy::Hybrid,
            }
        }
    }

    /// Mock reinforcement learning agent for task decomposition
    pub struct MockRLAgent {
        pub state_size: usize,
        pub action_size: usize,
        pub q_table: HashMap<String, Vec<f32>>,
        pub epsilon: f32,
        pub learning_rate: f32,
        pub discount_factor: f32,
        pub total_reward: f32,
    }

    impl MockRLAgent {
        pub fn new(state_size: usize, action_size: usize) -> Self {
            Self {
                state_size,
                action_size,
                q_table: HashMap::new(),
                epsilon: 0.1,
                learning_rate: 0.01,
                discount_factor: 0.95,
                total_reward: 0.0,
            }
        }

        pub fn get_state_key(&self, task: &Task) -> String {
            // Simple state representation
            format!("complexity_{:.1}_deps_{}_priority_{:?}", 
                task.complexity, task.dependencies.len(), task.priority)
        }

        pub fn choose_action(&mut self, task: &Task) -> usize {
            let state_key = self.get_state_key(task);
            
            // Epsilon-greedy action selection
            if rand::random::<f32>() < self.epsilon {
                // Random action
                rand::random::<usize>() % self.action_size
            } else {
                // Greedy action
                let q_values = self.q_table.get(&state_key)
                    .unwrap_or(&vec![0.0; self.action_size]);
                
                q_values.iter()
                    .enumerate()
                    .max_by(|(_, a), (_, b)| a.partial_cmp(b).unwrap())
                    .map(|(i, _)| i)
                    .unwrap_or(0)
            }
        }

        pub fn update_q_value(&mut self, task: &Task, action: usize, reward: f32, next_task: Option<&Task>) {
            let state_key = self.get_state_key(task);
            let q_values = self.q_table.entry(state_key.clone())
                .or_insert_with(|| vec![0.0; self.action_size]);
            
            let next_q_max = if let Some(next) = next_task {
                let next_state_key = self.get_state_key(next);
                self.q_table.get(&next_state_key)
                    .map(|q| q.iter().fold(0.0, |a, &b| a.max(b)))
                    .unwrap_or(0.0)
            } else {
                0.0
            };
            
            let target = reward + self.discount_factor * next_q_max;
            q_values[action] += self.learning_rate * (target - q_values[action]);
            
            self.total_reward += reward;
        }

        pub fn get_decomposition_action(&self, action: usize) -> DecompositionStrategy {
            match action {
                0 => DecompositionStrategy::Heuristic,
                1 => DecompositionStrategy::Neural,
                _ => DecompositionStrategy::Hybrid,
            }
        }
    }

    /// Mixture of Experts implementation for task decomposition
    pub struct MockMixtureOfExperts {
        pub experts: Vec<MockTransformerModel>,
        pub gating_network: MockDecisionNetwork,
        pub expert_weights: Vec<f32>,
    }

    impl MockMixtureOfExperts {
        pub fn new(num_experts: usize, vocab_size: usize, hidden_size: usize) -> Self {
            let experts = (0..num_experts).map(|i| {
                MockTransformerModel::new(
                    vocab_size,
                    hidden_size,
                    4 + i, // Varying number of layers
                    8,
                )
            }).collect();
            
            let gating_network = MockDecisionNetwork::new(7, vec![16, 8], num_experts);
            
            Self {
                experts,
                gating_network,
                expert_weights: vec![1.0 / num_experts as f32; num_experts],
            }
        }

        pub fn select_expert(&self, task: &Task) -> usize {
            let features = self.experts[0].encode_task(task);
            let expert_scores = self.gating_network.forward(&features);
            
            expert_scores.iter()
                .enumerate()
                .max_by(|(_, a), (_, b)| a.partial_cmp(b).unwrap())
                .map(|(i, _)| i)
                .unwrap_or(0)
        }

        pub fn decompose_with_expert(&self, task: &Task, expert_index: usize) -> Vec<Task> {
            let expert = &self.experts[expert_index];
            let encoded = expert.encode_task(task);
            expert.decode_subtasks(&encoded)
        }

        pub fn ensemble_decompose(&self, task: &Task) -> Vec<Task> {
            let mut all_decompositions = Vec::new();
            
            for (i, expert) in self.experts.iter().enumerate() {
                let encoded = expert.encode_task(task);
                let decomposition = expert.decode_subtasks(&encoded);
                all_decompositions.push((i, decomposition));
            }
            
            // Simple ensemble: use the expert with highest weight
            let best_expert = self.expert_weights.iter()
                .enumerate()
                .max_by(|(_, a), (_, b)| a.partial_cmp(b).unwrap())
                .map(|(i, _)| i)
                .unwrap_or(0);
            
            all_decompositions[best_expert].1.clone()
        }

        pub fn update_expert_weights(&mut self, task: &Task, expert_performances: &[f32]) {
            for (i, &performance) in expert_performances.iter().enumerate() {
                if i < self.expert_weights.len() {
                    self.expert_weights[i] = self.expert_weights[i] * 0.9 + performance * 0.1;
                }
            }
            
            // Normalize weights
            let sum: f32 = self.expert_weights.iter().sum();
            if sum > 0.0 {
                for weight in &mut self.expert_weights {
                    *weight /= sum;
                }
            }
        }
    }

    /// Tests for Transformer Model Validation
    #[tokio::test]
    async fn test_transformer_model_accuracy() {
        let mut model = MockTransformerModel::new(1000, 256, 6, 8);
        
        // Test encoding/decoding consistency
        let task = Task::new_test_task("transformer_test", 0.6);
        let encoded = model.encode_task(&task);
        let decoded = model.decode_subtasks(&encoded);
        
        assert_eq!(encoded.len(), 7); // Expected feature vector size
        assert!(!decoded.is_empty());
        
        // Test training step
        let expected_decomposition = vec![
            Task::new_test_task("expected_1", 0.3),
            Task::new_test_task("expected_2", 0.3),
        ];
        
        let accuracy = model.train_step(&task, &expected_decomposition);
        assert!(accuracy > 0.0);
        assert!(accuracy <= 1.0);
    }

    #[tokio::test]
    async fn test_transformer_bert_compatibility() {
        let model = MockTransformerModel::new(30522, 768, 12, 12); // BERT-base config
        
        // Test with various task complexities
        let tasks = vec![
            Task::new_test_task("bert_simple", 0.2),
            Task::new_test_task("bert_medium", 0.5),
            Task::new_test_task("bert_complex", 0.8),
        ];
        
        for task in tasks {
            let encoded = model.encode_task(&task);
            let decoded = model.decode_subtasks(&encoded);
            
            assert_eq!(encoded.len(), 7);
            assert!(!decoded.is_empty());
            
            // Complexity should influence number of subtasks
            let expected_subtasks = ((task.complexity * 5.0).max(1.0) as usize).min(5);
            assert_eq!(decoded.len(), expected_subtasks);
        }
    }

    #[tokio::test]
    async fn test_transformer_gpt_compatibility() {
        let model = MockTransformerModel::new(50257, 1024, 24, 16); // GPT-2 medium config
        
        // Test sequence generation capabilities
        let sequential_task = Task {
            id: "gpt_sequential".to_string(),
            name: "Sequential Processing Task".to_string(),
            description: "Task requiring sequential processing".to_string(),
            complexity: 0.7,
            dependencies: vec!["input_data".to_string()],
            priority: TaskPriority::High,
            estimated_duration: 1800,
            required_capabilities: vec!["sequential_processing".to_string()],
            resources: TaskResources {
                memory_mb: 4096,
                cpu_cores: 4.0,
                network_bandwidth: 1000,
                storage_gb: 100.0,
            },
        };
        
        let encoded = model.encode_task(&sequential_task);
        let decoded = model.decode_subtasks(&encoded);
        
        // Should generate sequential subtasks
        assert!(!decoded.is_empty());
        
        // Check sequential dependencies
        for i in 1..decoded.len() {
            assert!(!decoded[i].dependencies.is_empty());
        }
    }

    /// Tests for Decision Network Validation
    #[tokio::test]
    async fn test_decision_network_architecture() {
        let network = MockDecisionNetwork::new(7, vec![16, 8], 3);
        
        assert_eq!(network.input_size, 7);
        assert_eq!(network.hidden_layers, vec![16, 8]);
        assert_eq!(network.output_size, 3);
        assert_eq!(network.weights.len(), 3); // input->hidden1, hidden1->hidden2, hidden2->output
    }

    #[tokio::test]
    async fn test_decision_network_forward_pass() {
        let network = MockDecisionNetwork::new(7, vec![16, 8], 3);
        let input = vec![0.5, 0.6, 0.7, 0.8, 0.9, 0.1, 0.2];
        
        let output = network.forward(&input);
        
        assert_eq!(output.len(), 3);
        for &value in &output {
            assert!(value >= 0.0); // ReLU activation
        }
    }

    #[tokio::test]
    async fn test_decision_network_strategy_selection() {
        let network = MockDecisionNetwork::new(7, vec![16, 8], 3);
        
        // Test with different task types
        let test_tasks = vec![
            Task::new_test_task("simple", 0.2),
            Task::new_test_task("medium", 0.5),
            Task::new_test_task("complex", 0.8),
        ];
        
        for task in test_tasks {
            let features = vec![
                task.complexity,
                task.dependencies.len() as f32,
                task.estimated_duration as f32 / 3600.0,
                task.resources.memory_mb as f32 / 8192.0,
                task.resources.cpu_cores / 8.0,
                task.required_capabilities.len() as f32,
                match task.priority {
                    TaskPriority::Critical => 1.0,
                    TaskPriority::High => 0.8,
                    TaskPriority::Medium => 0.5,
                    TaskPriority::Low => 0.2,
                },
            ];
            
            let strategy = network.make_decision(&features);
            assert!(matches!(strategy, 
                DecompositionStrategy::Heuristic | 
                DecompositionStrategy::Neural | 
                DecompositionStrategy::Hybrid
            ));
        }
    }

    /// Tests for Reinforcement Learning Validation
    #[tokio::test]
    async fn test_rl_agent_initialization() {
        let agent = MockRLAgent::new(10, 3);
        
        assert_eq!(agent.state_size, 10);
        assert_eq!(agent.action_size, 3);
        assert!(agent.q_table.is_empty());
        assert!(agent.epsilon > 0.0);
        assert!(agent.learning_rate > 0.0);
        assert!(agent.discount_factor > 0.0);
    }

    #[tokio::test]
    async fn test_rl_agent_action_selection() {
        let mut agent = MockRLAgent::new(10, 3);
        let task = Task::new_test_task("rl_test", 0.5);
        
        // Test multiple action selections
        let mut actions = Vec::new();
        for _ in 0..10 {
            let action = agent.choose_action(&task);
            actions.push(action);
            assert!(action < 3);
        }
        
        // Should show some exploration (different actions)
        let unique_actions: std::collections::HashSet<_> = actions.into_iter().collect();
        assert!(unique_actions.len() > 0);
    }

    #[tokio::test]
    async fn test_rl_agent_learning() {
        let mut agent = MockRLAgent::new(10, 3);
        let task = Task::new_test_task("rl_learning", 0.6);
        
        // Simulate learning episodes
        for episode in 0..100 {
            let action = agent.choose_action(&task);
            let reward = if action == 1 { 1.0 } else { 0.0 }; // Reward action 1
            
            agent.update_q_value(&task, action, reward, None);
        }
        
        // Agent should learn to prefer action 1
        let state_key = agent.get_state_key(&task);
        let q_values = agent.q_table.get(&state_key).unwrap();
        
        assert!(q_values[1] > q_values[0]);
        assert!(q_values[1] > q_values[2]);
    }

    #[tokio::test]
    async fn test_rl_agent_convergence() {
        let mut agent = MockRLAgent::new(10, 3);
        let task = Task::new_test_task("rl_convergence", 0.7);
        
        let mut reward_history = Vec::new();
        
        // Train for multiple episodes
        for episode in 0..1000 {
            let action = agent.choose_action(&task);
            let reward = match action {
                0 => 0.5, // Heuristic gets medium reward
                1 => 1.0, // Neural gets high reward
                2 => 0.8, // Hybrid gets good reward
                _ => 0.0,
            };
            
            agent.update_q_value(&task, action, reward, None);
            
            if episode % 100 == 0 {
                reward_history.push(agent.total_reward / (episode + 1) as f32);
            }
        }
        
        // Reward should generally increase over time
        assert!(reward_history.len() > 2);
        let early_reward = reward_history[0];
        let late_reward = reward_history.last().unwrap();
        assert!(late_reward >= early_reward);
    }

    /// Tests for Mixture of Experts Validation
    #[tokio::test]
    async fn test_mixture_of_experts_initialization() {
        let moe = MockMixtureOfExperts::new(3, 1000, 256);
        
        assert_eq!(moe.experts.len(), 3);
        assert_eq!(moe.expert_weights.len(), 3);
        
        // Weights should be normalized
        let sum: f32 = moe.expert_weights.iter().sum();
        assert!((sum - 1.0).abs() < 0.001);
    }

    #[tokio::test]
    async fn test_mixture_of_experts_expert_selection() {
        let moe = MockMixtureOfExperts::new(3, 1000, 256);
        
        // Test expert selection for different tasks
        let tasks = vec![
            Task::new_test_task("moe_simple", 0.3),
            Task::new_test_task("moe_medium", 0.6),
            Task::new_test_task("moe_complex", 0.9),
        ];
        
        for task in tasks {
            let expert_index = moe.select_expert(&task);
            assert!(expert_index < moe.experts.len());
            
            let decomposition = moe.decompose_with_expert(&task, expert_index);
            assert!(!decomposition.is_empty());
        }
    }

    #[tokio::test]
    async fn test_mixture_of_experts_ensemble() {
        let moe = MockMixtureOfExperts::new(3, 1000, 256);
        let task = Task::new_test_task("moe_ensemble", 0.7);
        
        let ensemble_result = moe.ensemble_decompose(&task);
        assert!(!ensemble_result.is_empty());
        
        // Compare with individual expert results
        let expert_0_result = moe.decompose_with_expert(&task, 0);
        let expert_1_result = moe.decompose_with_expert(&task, 1);
        let expert_2_result = moe.decompose_with_expert(&task, 2);
        
        // Ensemble should pick one of the expert results
        assert!(
            ensemble_result.len() == expert_0_result.len() ||
            ensemble_result.len() == expert_1_result.len() ||
            ensemble_result.len() == expert_2_result.len()
        );
    }

    #[tokio::test]
    async fn test_mixture_of_experts_adaptation() {
        let mut moe = MockMixtureOfExperts::new(3, 1000, 256);
        let task = Task::new_test_task("moe_adaptation", 0.8);
        
        // Simulate expert performance feedback
        let performances = vec![0.9, 0.6, 0.8]; // Expert 0 performs best
        moe.update_expert_weights(&task, &performances);
        
        // Expert 0 should have higher weight
        assert!(moe.expert_weights[0] > moe.expert_weights[1]);
        assert!(moe.expert_weights[0] > moe.expert_weights[2]);
        
        // Weights should still be normalized
        let sum: f32 = moe.expert_weights.iter().sum();
        assert!((sum - 1.0).abs() < 0.001);
    }

    /// Integration tests for neural architectures
    #[tokio::test]
    async fn test_neural_architecture_integration() {
        let transformer = MockTransformerModel::new(1000, 256, 6, 8);
        let decision_network = MockDecisionNetwork::new(7, vec![16, 8], 3);
        let mut rl_agent = MockRLAgent::new(10, 3);
        let moe = MockMixtureOfExperts::new(3, 1000, 256);
        
        let task = Task::new_test_task("integration_test", 0.7);
        
        // Test complete pipeline
        let features = transformer.encode_task(&task);
        let strategy = decision_network.make_decision(&features);
        let action = rl_agent.choose_action(&task);
        let expert_index = moe.select_expert(&task);
        
        // All components should work together
        assert_eq!(features.len(), 7);
        assert!(matches!(strategy, 
            DecompositionStrategy::Heuristic | 
            DecompositionStrategy::Neural | 
            DecompositionStrategy::Hybrid
        ));
        assert!(action < 3);
        assert!(expert_index < 3);
    }

    #[tokio::test]
    async fn test_neural_architecture_performance() {
        let transformer = MockTransformerModel::new(1000, 256, 6, 8);
        let mut profiler = crate::test_utils::PerformanceProfiler::new();
        
        // Test performance with various task sizes
        let tasks = vec![
            Task::new_test_task("perf_small", 0.2),
            Task::new_test_task("perf_medium", 0.5),
            Task::new_test_task("perf_large", 0.8),
        ];
        
        for task in tasks {
            let _result = profiler.time_async("neural_encoding", || async {
                transformer.encode_task(&task)
            }).await;
        }
        
        // Check performance statistics
        let stats = profiler.get_stats("neural_encoding").unwrap();
        assert!(stats.count == 3);
        assert!(stats.mean_us > 0.0);
        
        // Performance should be reasonable
        assert!(stats.max_us < 10000); // Less than 10ms
    }
}